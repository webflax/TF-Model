{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "H1F56lS_93TS",
   "metadata": {
    "id": "H1F56lS_93TS"
   },
   "source": [
    "# lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94bd2b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:22:29.385315Z",
     "iopub.status.busy": "2021-11-09T08:22:29.384708Z",
     "iopub.status.idle": "2021-11-09T08:22:37.325011Z",
     "shell.execute_reply": "2021-11-09T08:22:37.325524Z",
     "shell.execute_reply.started": "2021-11-09T08:11:07.476205Z"
    },
    "id": "94bd2b0c",
    "papermill": {
     "duration": 7.986193,
     "end_time": "2021-11-09T08:22:37.325835",
     "exception": false,
     "start_time": "2021-11-09T08:22:29.339642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# import os\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "# import math\n",
    "# import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score \n",
    "# from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
    "# from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "\n",
    "# from itertools import product\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, ConvLSTM1D, Conv1D, Concatenate, Bidirectional\n",
    "from tensorflow.keras.layers import LSTM , BatchNormalization, AveragePooling1D, Flatten, GRU, SimpleRNN\n",
    "\n",
    "# from itertools import cycle\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pgHmOgKxDYAo",
   "metadata": {
    "id": "pgHmOgKxDYAo"
   },
   "source": [
    "# creat data lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bRproMPf8Wg8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRproMPf8Wg8",
    "outputId": "82b810c8-52a9-433b-e604-43cc2dd2fc8d"
   },
   "outputs": [],
   "source": [
    "with open('btc_stan.moh','rb') as f :\n",
    "    btc = pickle.load(f)\n",
    "btc=(btc+2)/4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2298a344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21bccba5490>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.plot(btc.close_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5pihpUZtif5G",
   "metadata": {
    "id": "5pihpUZtif5G"
   },
   "outputs": [],
   "source": [
    "look_back_ = 90\n",
    "prediction_days = 60\n",
    "future_ = 30\n",
    "\n",
    "\n",
    "df_train_= btc[:][:len(btc)-(prediction_days+future_)]#values.reshape(-1,1)\n",
    "\n",
    "df_test_= btc[:][len(btc)-(prediction_days+future_+look_back_-1):len(btc)-future_]#.values.reshape(-1,1)\n",
    "\n",
    "future_ = btc[:][len(btc)-(future_+look_back_-1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "yoYvHWVXFNJf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yoYvHWVXFNJf",
    "outputId": "7f68bc07-1bf0-4d70-9542-c1fae86702a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['close_log', 'close', 'target_log', 'target'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vA-pnvD6EXyv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "vA-pnvD6EXyv",
    "outputId": "f44775b1-bdb8-4067-cff7-01c45fc2b817"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close_log</th>\n",
       "      <th>close</th>\n",
       "      <th>target_log</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-05-04</th>\n",
       "      <td>0.920746</td>\n",
       "      <td>1.239421</td>\n",
       "      <td>0.922647</td>\n",
       "      <td>1.247350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-05</th>\n",
       "      <td>0.922647</td>\n",
       "      <td>1.249706</td>\n",
       "      <td>0.925998</td>\n",
       "      <td>1.265423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            close_log     close  target_log    target\n",
       "date                                                 \n",
       "2021-05-04   0.920746  1.239421    0.922647  1.247350\n",
       "2021-05-05   0.922647  1.249706    0.925998  1.265423"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = ['close_log', 'close', 'target_log', 'target']\n",
    "df_train = df_train_[:][col]\n",
    "df_test = df_test_[:][col]\n",
    "future = future_[:][col]\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e1e5fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:24:00.120271Z",
     "iopub.status.busy": "2021-11-09T08:24:00.119252Z",
     "iopub.status.idle": "2021-11-09T08:24:00.134344Z",
     "shell.execute_reply": "2021-11-09T08:24:00.133732Z",
     "shell.execute_reply.started": "2021-11-09T07:53:28.462429Z"
    },
    "id": "f3e1e5fa",
    "papermill": {
     "duration": 0.066879,
     "end_time": "2021-11-09T08:24:00.134486",
     "exception": false,
     "start_time": "2021-11-09T08:24:00.067607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # many to one\n",
    "\n",
    "# from collections import deque\n",
    "# def dataset_generator_lstm(dataset, look_back=look_back_):\n",
    "#   sequential_data = [] \n",
    "#   prev_days = deque(maxlen=look_back)\n",
    "#   for i in dataset.values:\n",
    "#     prev_days.append([n for n in i[:-1]])\n",
    "#     if len(prev_days) == look_back:  \n",
    "#       sequential_data.append([np.array(prev_days), i[-1]])\n",
    "#   dataX=[]\n",
    "#   dataY=[]\n",
    "#   for seq, target in sequential_data: \n",
    "#     dataX.append(seq) \n",
    "#     dataY.append(target) \n",
    "#   return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# trainX, trainY = dataset_generator_lstm(df_train)\n",
    "\n",
    "# testX, testY = dataset_generator_lstm(df_test)\n",
    "# # trainX, trainY = trainX[:1740], trainY[:1740]\n",
    "\n",
    "# futureX, futureY = dataset_generator_lstm(future)\n",
    "\n",
    "# print(\"trainX: \", trainX.shape)\n",
    "# print(\"trainY: \", trainY.shape)\n",
    "# print(\"testX: \", testX.shape)\n",
    "# print(\"testY\", testY.shape)\n",
    "# print(\"futureX: \", futureX.shape)\n",
    "# print(\"futureY\", futureY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "SwFdzjKjUhWT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SwFdzjKjUhWT",
    "outputId": "7fcff1d1-91a3-49e2-c9fb-8a7ddc950b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX:  (2016, 90, 1)\n",
      "trainY:  (2016,)\n",
      "trainX_log:  (2016, 90, 1)\n",
      "trainY_log:  (2016,)\n",
      "testX:  (60, 90, 1)\n",
      "testY (60,)\n",
      "testX_log:  (60, 90, 1)\n",
      "testY_log (60,)\n",
      "futureX:  (30, 90, 1)\n",
      "futureY (30,)\n",
      "futureX_log:  (30, 90, 1)\n",
      "futureY_log (30,)\n"
     ]
    }
   ],
   "source": [
    "# one to one\n",
    "\n",
    "from collections import deque\n",
    "def dataset_generator_lstm(dataset, look_back=look_back_):\n",
    "    sequential_data = [] \n",
    "    sequential_data_log = [] \n",
    "    prev_days = deque(maxlen=look_back)\n",
    "    prev_days_log = deque(maxlen=look_back)\n",
    "\n",
    "    for i in dataset.values:\n",
    "        prev_days_log.append(i[0])\n",
    "        prev_days.append(i[1])\n",
    "\n",
    "        if len(prev_days) == look_back:  \n",
    "            sequential_data_log.append([np.array(prev_days), i[-1]])\n",
    "            sequential_data.append([np.array(prev_days), i[-2]])\n",
    "      \n",
    "      \n",
    "    dataX_log=[]\n",
    "    dataY_log=[]\n",
    "\n",
    "    dataX=[]\n",
    "    dataY=[]\n",
    "\n",
    "    for seq, target in sequential_data_log: \n",
    "        dataX_log.append(seq) \n",
    "        dataY_log.append(target) \n",
    "\n",
    "    for seq, target in sequential_data: \n",
    "        dataX.append(seq) \n",
    "        dataY.append(target) \n",
    "\n",
    "    dataX_log = tf.expand_dims(dataX_log,axis=2)\n",
    "    dataX = tf.expand_dims(dataX,axis=2)\n",
    "\n",
    "    return np.array(dataX_log), np.array(dataY_log), np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX_log, trainY_log, trainX, trainY = dataset_generator_lstm(df_train)\n",
    "\n",
    "testX_log, testY_log, testX, testY = dataset_generator_lstm(df_test)\n",
    "# trainX, trainY = trainX[:1740], trainY[:1740]\n",
    "\n",
    "futureX_log, futureY_log, futureX, futureY = dataset_generator_lstm(future)\n",
    "\n",
    "print(\"trainX: \", trainX.shape)\n",
    "print(\"trainY: \", trainY.shape)\n",
    "print(\"trainX_log: \", trainX_log.shape)\n",
    "print(\"trainY_log: \", trainY_log.shape)\n",
    "print(\"testX: \", testX.shape)\n",
    "print(\"testY\", testY.shape)\n",
    "print(\"testX_log: \", testX_log.shape)\n",
    "print(\"testY_log\", testY_log.shape)\n",
    "print(\"futureX: \", futureX.shape)\n",
    "print(\"futureY\", futureY.shape)\n",
    "print(\"futureX_log: \", futureX_log.shape)\n",
    "print(\"futureY_log\", futureY_log.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lLbwlWzN8P6-",
   "metadata": {
    "id": "lLbwlWzN8P6-"
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c247041",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T08:24:01.286885Z",
     "iopub.status.busy": "2021-11-09T08:24:01.285918Z",
     "iopub.status.idle": "2021-11-09T08:24:01.746820Z",
     "shell.execute_reply": "2021-11-09T08:24:01.747339Z",
     "shell.execute_reply.started": "2021-11-09T07:53:28.553350Z"
    },
    "id": "0c247041",
    "outputId": "996acf98-8edd-4b52-8bcb-de08823f9132",
    "papermill": {
     "duration": 0.51592,
     "end_time": "2021-11-09T08:24:01.747513",
     "exception": false,
     "start_time": "2021-11-09T08:24:01.231593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 90, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 90, 30)       930         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 90, 60)       900         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 90, 90)       720         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 90, 30)       930         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 90, 60)       900         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 90, 90)       720         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 90, 180)      0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 90, 180)      0           ['conv1d_9[0][0]',               \n",
      "                                                                  'conv1d_10[0][0]',              \n",
      "                                                                  'conv1d_11[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling1d (AveragePool  (None, 30, 180)     0           ['concatenate[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " average_pooling1d_3 (AveragePo  (None, 30, 180)     0           ['concatenate_3[0][0]']          \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 30, 180)     720         ['average_pooling1d[0][0]']      \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 30, 180)     720         ['average_pooling1d_3[0][0]']    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 30, 180)      0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 30, 180)      0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 30, 90)       340290      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 30, 180)      453780      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 30, 270)      340470      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 30, 90)       340290      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 30, 180)      453780      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 30, 270)      340470      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 30, 540)      0           ['conv1d_3[0][0]',               \n",
      "                                                                  'conv1d_4[0][0]',               \n",
      "                                                                  'conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 30, 540)      0           ['conv1d_12[0][0]',              \n",
      "                                                                  'conv1d_13[0][0]',              \n",
      "                                                                  'conv1d_14[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling1d_1 (AveragePo  (None, 15, 540)     0           ['concatenate_1[0][0]']          \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " average_pooling1d_4 (AveragePo  (None, 15, 540)     0           ['concatenate_4[0][0]']          \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 15, 540)     2160        ['average_pooling1d_1[0][0]']    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 15, 540)     2160        ['average_pooling1d_4[0][0]']    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 15, 540)      0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 15, 540)      0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 15, 270)      3062070     ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 15, 360)      2721960     ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 15, 450)      1701450     ['dropout_1[0][0]']              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv1d_15 (Conv1D)             (None, 15, 270)      3062070     ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 15, 360)      2721960     ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 15, 450)      1701450     ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 15, 1080)     0           ['conv1d_6[0][0]',               \n",
      "                                                                  'conv1d_7[0][0]',               \n",
      "                                                                  'conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 15, 1080)     0           ['conv1d_15[0][0]',              \n",
      "                                                                  'conv1d_16[0][0]',              \n",
      "                                                                  'conv1d_17[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling1d_2 (AveragePo  (None, 7, 1080)     0           ['concatenate_2[0][0]']          \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " average_pooling1d_5 (AveragePo  (None, 7, 1080)     0           ['concatenate_5[0][0]']          \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 7, 1080)     4320        ['average_pooling1d_2[0][0]']    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 7, 1080)     4320        ['average_pooling1d_5[0][0]']    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 7, 1080)      0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 7, 1080)      0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 7, 2160)      0           ['dropout_2[0][0]',              \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 7, 1024)      10948608    ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirectional  (None, 7, 1024)     8214528     ['concatenate_6[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_10 (Bidirectiona  (None, 7, 1024)     8214528     ['concatenate_6[0][0]']          \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 7, 1024)     4096        ['bidirectional[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 7, 1024)     4096        ['bidirectional_5[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 7, 1024)     4096        ['bidirectional_10[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 7, 1024)      0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 7, 1024)      0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 7, 1024)      0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 7, 512)      2623488     ['dropout_6[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_6 (Bidirectional  (None, 7, 512)      1969152     ['dropout_11[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_11 (Bidirectiona  (None, 7, 512)      1969152     ['dropout_16[0][0]']             \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 7, 512)      2048        ['bidirectional_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 7, 512)      2048        ['bidirectional_6[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 7, 512)      2048        ['bidirectional_11[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 7, 512)       0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 7, 512)       0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 7, 512)       0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 7, 1024)     4198400     ['dropout_7[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bidirectional_7 (Bidirectional  (None, 7, 1024)     3151872     ['dropout_12[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_12 (Bidirectiona  (None, 7, 1024)     3151872     ['dropout_17[0][0]']             \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 7, 1024)     4096        ['bidirectional_2[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 7, 1024)     4096        ['bidirectional_7[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 7, 1024)     4096        ['bidirectional_12[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 7, 1024)      0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 7, 1024)      0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 7, 1024)      0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 7, 512)      2623488     ['dropout_8[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_8 (Bidirectional  (None, 7, 512)      1969152     ['dropout_13[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_13 (Bidirectiona  (None, 7, 512)      1969152     ['dropout_18[0][0]']             \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 7, 512)      2048        ['bidirectional_3[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 7, 512)      2048        ['bidirectional_8[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 7, 512)      2048        ['bidirectional_13[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 7, 512)       0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 7, 512)       0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 7, 512)       0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirectional  (None, 7, 1024)     4198400     ['dropout_9[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_9 (Bidirectional  (None, 7, 1024)     3151872     ['dropout_14[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_14 (Bidirectiona  (None, 7, 1024)     3151872     ['dropout_19[0][0]']             \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 7168)         0           ['bidirectional_4[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 7168)         0           ['bidirectional_9[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 7168)         0           ['bidirectional_14[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 7168)        28672       ['flatten[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 7168)        28672       ['flatten_1[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 7168)        28672       ['flatten_2[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 7168)         0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 7168)         0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 7168)         0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 21504)        0           ['dropout_10[0][0]',             \n",
      "                                                                  'dropout_15[0][0]',             \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2048)         44042240    ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2048)         44042240    ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_6 (Dense)                (None, 2048)         44042240    ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 2048)        8192        ['dense[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 2048)        8192        ['dense_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 2048)        8192        ['dense_6[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 2048)         0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 2048)         0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 2048)         0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1024)         2098176     ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1024)         2098176     ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1024)         2098176     ['dropout_27[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 1024)        4096        ['dense_1[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 1024)        4096        ['dense_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 1024)        4096        ['dense_7[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 1024)         0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 1024)         0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 1024)         0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          524800      ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 512)          524800      ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 512)          524800      ['dropout_28[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 512)         2048        ['dense_2[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 512)         2048        ['dense_5[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 512)         2048        ['dense_8[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 512)          0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 512)          0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 512)          0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 1536)         0           ['dropout_23[0][0]',             \n",
      "                                                                  'dropout_26[0][0]',             \n",
      "                                                                  'dropout_29[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 2048)         3147776     ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 2048)        8192        ['dense_9[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 2048)         0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 512)          1049088     ['dropout_31[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 512)         2048        ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 512)          0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 223,133,716\n",
      "Trainable params: 223,038,452\n",
      "Non-trainable params: 95,264\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "input1 = Input(shape=trainX_log.shape[1:])\n",
    "\n",
    "conv111 = Conv1D(filters=30,kernel_size=30,padding=\"same\",activation=\"relu\")(input1)\n",
    "conv112 = Conv1D(filters=60,kernel_size=14,padding=\"same\",activation=\"relu\")(input1)\n",
    "conv113 = Conv1D(filters=90,kernel_size=7,padding=\"same\",activation=\"relu\")(input1)\n",
    "\n",
    "conv11 = Concatenate(axis=2)([conv111, conv112, conv113])\n",
    "\n",
    "pool11 = AveragePooling1D(pool_size=61, strides=1, padding=\"valid\", data_format=\"channels_last\")(conv11)\n",
    "\n",
    "norm11 = BatchNormalization()(pool11)\n",
    "drop11 = Dropout(rate=0.3)(norm11)\n",
    "\n",
    "conv121 = Conv1D(filters=90,kernel_size=21,padding=\"same\",activation=\"relu\")(drop11)\n",
    "conv122 = Conv1D(filters=180,kernel_size=14,padding=\"same\",activation=\"relu\")(drop11)\n",
    "conv123 = Conv1D(filters=270,kernel_size=7,padding=\"same\",activation=\"relu\")(drop11)\n",
    "\n",
    "conv12 = Concatenate(axis=2)([conv121, conv122, conv123])\n",
    "\n",
    "pool12 = AveragePooling1D(pool_size=16, strides=1, padding=\"valid\", data_format=\"channels_last\")(conv12)\n",
    "\n",
    "norm12 = BatchNormalization()(pool12)\n",
    "drop12 = Dropout(rate=0.3)(norm12)\n",
    "\n",
    "conv131 = Conv1D(filters=270,kernel_size=21,padding=\"same\",activation=\"relu\")(drop12)\n",
    "conv132 = Conv1D(filters=360,kernel_size=14,padding=\"same\",activation=\"relu\")(drop12)\n",
    "conv133 = Conv1D(filters=450,kernel_size=7,padding=\"same\",activation=\"relu\")(drop12)\n",
    "\n",
    "conv13 = Concatenate(axis=2)([conv131, conv132, conv133])\n",
    "\n",
    "pool13 = AveragePooling1D(pool_size=9, strides=1, padding=\"valid\", data_format=\"channels_last\")(conv13)\n",
    "\n",
    "norm13 = BatchNormalization()(pool13)\n",
    "drop13 = Dropout(rate=0.3)(norm13)\n",
    "\n",
    "\n",
    "\n",
    "conv211 = Conv1D(filters=30,kernel_size=30,padding=\"same\",activation=\"relu\")(input1)\n",
    "conv212 = Conv1D(filters=60,kernel_size=14,padding=\"same\",activation=\"relu\")(input1)\n",
    "conv213 = Conv1D(filters=90,kernel_size=7,padding=\"same\",activation=\"relu\")(input1)\n",
    "\n",
    "conv21 = Concatenate(axis=2)([conv211, conv212, conv213])\n",
    "\n",
    "pool21 = AveragePooling1D(pool_size=61, strides=1, padding=\"valid\", data_format=\"channels_last\")(conv21)\n",
    "\n",
    "norm21 = BatchNormalization()(pool21)\n",
    "drop21 = Dropout(rate=0.3)(norm21)\n",
    "\n",
    "conv221 = Conv1D(filters=90,kernel_size=21,padding=\"same\",activation=\"relu\")(drop21)\n",
    "conv222 = Conv1D(filters=180,kernel_size=14,padding=\"same\",activation=\"relu\")(drop21)\n",
    "conv223 = Conv1D(filters=270,kernel_size=7,padding=\"same\",activation=\"relu\")(drop21)\n",
    "\n",
    "conv22 = Concatenate(axis=2)([conv221, conv222, conv223])\n",
    "\n",
    "pool22 = AveragePooling1D(pool_size=16, strides=1, padding=\"valid\", data_format=\"channels_last\")(conv22)\n",
    "\n",
    "norm22 = BatchNormalization()(pool22)\n",
    "drop22 = Dropout(rate=0.3)(norm22)\n",
    "\n",
    "conv231 = Conv1D(filters=270,kernel_size=21,padding=\"same\",activation=\"relu\")(drop22)\n",
    "conv232 = Conv1D(filters=360,kernel_size=14,padding=\"same\",activation=\"relu\")(drop22)\n",
    "conv233 = Conv1D(filters=450,kernel_size=7,padding=\"same\",activation=\"relu\")(drop22)\n",
    "\n",
    "conv23 = Concatenate(axis=2)([conv231, conv232, conv233])\n",
    "\n",
    "pool23 = AveragePooling1D(pool_size=9, strides=1, padding=\"valid\", data_format=\"channels_last\")(conv23)\n",
    "\n",
    "norm23 = BatchNormalization()(pool23)\n",
    "drop23 = Dropout(rate=0.3)(norm23)\n",
    "\n",
    "drop3 = Concatenate(axis=2)([drop13, drop23])\n",
    "\n",
    "\n",
    "\n",
    "rnn11 = Bidirectional(layer=LSTM(units = 512, activation = 'relu', return_sequences=True))(drop3)\n",
    "\n",
    "norm14 = BatchNormalization()(rnn11)\n",
    "drop14 = Dropout(rate=0.3)(norm14)\n",
    "\n",
    "rnn12 = Bidirectional(LSTM(units = 256, activation = 'relu', return_sequences=True))(drop14)\n",
    "\n",
    "norm15 = BatchNormalization()(rnn12)\n",
    "drop15 = Dropout(rate=0.3)(norm15)\n",
    "\n",
    "rnn13 = Bidirectional(LSTM(units = 512, activation = 'relu', return_sequences=True))(drop15)\n",
    "\n",
    "norm15_2 = BatchNormalization()(rnn13)\n",
    "drop15_2 = Dropout(rate=0.2)(norm15_2)\n",
    "\n",
    "rnn14 = Bidirectional(LSTM(units = 256, activation = 'relu', return_sequences=True))(drop15_2)\n",
    "\n",
    "norm16 = BatchNormalization()(rnn14)\n",
    "drop16 = Dropout(rate=0.2)(norm16)\n",
    "\n",
    "rnn15 = Bidirectional(LSTM(units = 512, activation = 'relu', return_sequences=True))(drop16)\n",
    "\n",
    "flatten = Flatten()(rnn15)\n",
    "norm17 = BatchNormalization()(flatten)\n",
    "drop17 = Dropout(rate=0.3)(norm17)\n",
    "\n",
    "\n",
    "\n",
    "rnn21 = Bidirectional(layer=GRU(units = 512, activation = 'relu', return_sequences=True))(drop3)\n",
    "\n",
    "norm24 = BatchNormalization()(rnn21)\n",
    "drop24 = Dropout(rate=0.3)(norm24)\n",
    "\n",
    "rnn22 = Bidirectional(GRU(units = 256, activation = 'relu', return_sequences=True))(drop24)\n",
    "\n",
    "norm25 = BatchNormalization()(rnn22)\n",
    "drop25 = Dropout(rate=0.3)(norm25)\n",
    "\n",
    "rnn23 = Bidirectional(GRU(units = 512, activation = 'relu', return_sequences=True))(drop25)\n",
    "\n",
    "norm25_2 = BatchNormalization()(rnn23)\n",
    "drop25_2 = Dropout(rate=0.2)(norm25_2)\n",
    "\n",
    "rnn24 = Bidirectional(GRU(units = 256, activation = 'relu', return_sequences=True))(drop25_2)\n",
    "\n",
    "norm26 = BatchNormalization()(rnn24)\n",
    "drop26 = Dropout(rate=0.2)(norm26)\n",
    "\n",
    "rnn25 = Bidirectional(GRU(units = 512, activation = 'relu', return_sequences=True))(drop26)\n",
    "\n",
    "flatten2 = Flatten()(rnn25)\n",
    "norm27 = BatchNormalization()(flatten2)\n",
    "drop27 = Dropout(rate=0.3)(norm27)\n",
    "\n",
    "\n",
    "\n",
    "rnn31 = Bidirectional(layer=GRU(units = 512, activation = 'relu', return_sequences=True))(drop3)\n",
    "\n",
    "norm34 = BatchNormalization()(rnn31)\n",
    "drop34 = Dropout(rate=0.3)(norm34)\n",
    "\n",
    "rnn32 = Bidirectional(GRU(units = 256, activation = 'relu', return_sequences=True))(drop34)\n",
    "\n",
    "norm35 = BatchNormalization()(rnn32)\n",
    "drop35 = Dropout(rate=0.3)(norm35)\n",
    "\n",
    "rnn33 = Bidirectional(GRU(units = 512, activation = 'relu', return_sequences=True))(drop35)\n",
    "\n",
    "norm35_2 = BatchNormalization()(rnn33)\n",
    "drop35_2 = Dropout(rate=0.2)(norm35_2)\n",
    "\n",
    "rnn34 = Bidirectional(GRU(units = 256, activation = 'relu', return_sequences=True))(drop35_2)\n",
    "\n",
    "norm36 = BatchNormalization()(rnn34)\n",
    "drop36 = Dropout(rate=0.2)(norm36)\n",
    "\n",
    "rnn35 = Bidirectional(GRU(units = 512, activation = 'relu', return_sequences=True))(drop36)\n",
    "\n",
    "flatten3 = Flatten()(rnn35)\n",
    "norm37 = BatchNormalization()(flatten3)\n",
    "drop37 = Dropout(rate=0.3)(norm37)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "drop7 = Concatenate()([drop17, drop27, drop37])\n",
    "\n",
    "\n",
    "\n",
    "dense11 = Dense(units = 2048, activation='relu')(drop7)\n",
    "\n",
    "norm18 = BatchNormalization()(dense11)\n",
    "drop18 = Dropout(rate=0.3)(norm18)\n",
    "\n",
    "dense12 = Dense(units = 1024, activation='relu')(drop18)\n",
    "\n",
    "norm19 = BatchNormalization()(dense12)\n",
    "drop19 = Dropout(rate=0.2)(norm19)\n",
    "\n",
    "dense113 = Dense(units = 512, activation='relu')(drop19)\n",
    "\n",
    "norm110 = BatchNormalization()(dense113)\n",
    "drop110 = Dropout(rate=0.2)(norm110)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dense21 = Dense(units = 2048, activation='relu')(drop7)\n",
    "\n",
    "norm28 = BatchNormalization()(dense21)\n",
    "drop28 = Dropout(rate=0.3)(norm28)\n",
    "\n",
    "dense22 = Dense(units = 1024, activation='relu')(drop28)\n",
    "\n",
    "norm29 = BatchNormalization()(dense22)\n",
    "drop29 = Dropout(rate=0.2)(norm29)\n",
    "\n",
    "dense213 = Dense(units = 512, activation='relu')(drop29)\n",
    "\n",
    "norm210 = BatchNormalization()(dense213)\n",
    "drop210 = Dropout(rate=0.2)(norm210)\n",
    "\n",
    "\n",
    "\n",
    "dense31 = Dense(units = 2048, activation='relu')(drop7)\n",
    "\n",
    "norm38 = BatchNormalization()(dense31)\n",
    "drop38 = Dropout(rate=0.3)(norm38)\n",
    "\n",
    "dense32 = Dense(units = 1024, activation='relu')(drop38)\n",
    "\n",
    "norm39 = BatchNormalization()(dense32)\n",
    "drop39 = Dropout(rate=0.2)(norm39)\n",
    "\n",
    "dense313 = Dense(units = 512, activation='relu')(drop39)\n",
    "\n",
    "norm310 = BatchNormalization()(dense313)\n",
    "drop310 = Dropout(rate=0.2)(norm310)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "drop81 = Concatenate()([drop110, drop210, drop310])\n",
    "\n",
    "\n",
    "\n",
    "dense1 = Dense(units = 2048, activation='relu')(drop81)\n",
    "\n",
    "norm8 = BatchNormalization()(dense1)\n",
    "drop8 = Dropout(rate=0.3)(norm8)\n",
    "\n",
    "dense2 = Dense(units = 1024, activation='relu')(drop8)\n",
    "\n",
    "norm10 = BatchNormalization()(dense1)\n",
    "drop10 = Dropout(rate=0.2)(norm10)\n",
    "\n",
    "dense3 = Dense(units = 512, activation='relu')(drop10)\n",
    "\n",
    "norm11 = BatchNormalization()(dense3)\n",
    "drop11 = Dropout(rate=0.2)(norm11)\n",
    "\n",
    "\n",
    "output = Dense(units = 1)(drop10)\n",
    "\n",
    "model = Model(inputs=input1, outputs=drop11)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "dot_img_file = 'V0.8.png'\n",
    "tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "xG7W--s0vF-C",
   "metadata": {
    "id": "xG7W--s0vF-C"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# tf.keras.optimizers.SGD()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD() , loss = 'mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "gHZ7ed-z9CAS",
   "metadata": {
    "id": "gHZ7ed-z9CAS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('my_best_model_v08.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "737a2ede",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T08:24:01.859939Z",
     "iopub.status.busy": "2021-11-09T08:24:01.859233Z",
     "iopub.status.idle": "2021-11-09T08:24:34.491023Z",
     "shell.execute_reply": "2021-11-09T08:24:34.491550Z",
     "shell.execute_reply.started": "2021-11-09T07:53:29.074954Z"
    },
    "id": "737a2ede",
    "outputId": "c50e8ef6-a013-476d-8d82-078fec829d37",
    "papermill": {
     "duration": 32.69245,
     "end_time": "2021-11-09T08:24:34.491782",
     "exception": false,
     "start_time": "2021-11-09T08:24:01.799332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7598 \n",
      "Epoch 1: loss improved from inf to 0.75984, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 98s 41s/step - loss: 0.7598 - val_loss: 0.9938 - lr: 0.0100\n",
      "Epoch 2/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7586 \n",
      "Epoch 2: loss improved from 0.75984 to 0.75864, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 71s 37s/step - loss: 0.7586 - val_loss: 0.9596 - lr: 0.0100\n",
      "Epoch 3/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7583 \n",
      "Epoch 3: loss improved from 0.75864 to 0.75827, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 69s 36s/step - loss: 0.7583 - val_loss: 0.9386 - lr: 0.0100\n",
      "Epoch 4/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7581 \n",
      "Epoch 4: loss improved from 0.75827 to 0.75810, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 68s 36s/step - loss: 0.7581 - val_loss: 0.9065 - lr: 0.0100\n",
      "Epoch 5/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7595 \n",
      "Epoch 5: loss did not improve from 0.75810\n",
      "2/2 [==============================] - 66s 33s/step - loss: 0.7595 - val_loss: 0.9933 - lr: 0.0100\n",
      "Epoch 6/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7573 \n",
      "Epoch 6: loss improved from 0.75810 to 0.75729, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 64s 33s/step - loss: 0.7573 - val_loss: 0.9769 - lr: 0.0100\n",
      "Epoch 7/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7583 \n",
      "Epoch 7: loss did not improve from 0.75729\n",
      "2/2 [==============================] - 65s 34s/step - loss: 0.7583 - val_loss: 1.0098 - lr: 0.0100\n",
      "Epoch 8/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7586 \n",
      "Epoch 8: loss did not improve from 0.75729\n",
      "2/2 [==============================] - 66s 33s/step - loss: 0.7586 - val_loss: 0.9536 - lr: 0.0100\n",
      "Epoch 9/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7582 \n",
      "Epoch 9: loss did not improve from 0.75729\n",
      "2/2 [==============================] - 65s 32s/step - loss: 0.7582 - val_loss: 0.9531 - lr: 0.0100\n",
      "Epoch 10/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7576 \n",
      "Epoch 10: loss did not improve from 0.75729\n",
      "2/2 [==============================] - 63s 32s/step - loss: 0.7576 - val_loss: 0.9120 - lr: 0.0100\n",
      "Epoch 11/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7583 \n",
      "Epoch 11: loss did not improve from 0.75729\n",
      "2/2 [==============================] - 67s 33s/step - loss: 0.7583 - val_loss: 0.9200 - lr: 0.0100\n",
      "Epoch 12/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7575 \n",
      "Epoch 12: loss did not improve from 0.75729\n",
      "2/2 [==============================] - 70s 35s/step - loss: 0.7575 - val_loss: 0.8178 - lr: 0.0100\n",
      "Epoch 13/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7572 \n",
      "Epoch 13: loss improved from 0.75729 to 0.75721, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 72s 37s/step - loss: 0.7572 - val_loss: 0.8069 - lr: 0.0100\n",
      "Epoch 14/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7575 \n",
      "Epoch 14: loss did not improve from 0.75721\n",
      "2/2 [==============================] - 67s 33s/step - loss: 0.7575 - val_loss: 0.9118 - lr: 0.0100\n",
      "Epoch 15/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7566 \n",
      "Epoch 15: loss improved from 0.75721 to 0.75664, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 72s 36s/step - loss: 0.7566 - val_loss: 0.9063 - lr: 0.0100\n",
      "Epoch 16/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7573 \n",
      "Epoch 16: loss did not improve from 0.75664\n",
      "2/2 [==============================] - 67s 34s/step - loss: 0.7573 - val_loss: 0.8843 - lr: 0.0100\n",
      "Epoch 17/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7567 \n",
      "Epoch 17: loss did not improve from 0.75664\n",
      "2/2 [==============================] - 68s 34s/step - loss: 0.7567 - val_loss: 0.9380 - lr: 0.0100\n",
      "Epoch 18/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7569 \n",
      "Epoch 18: loss did not improve from 0.75664\n",
      "2/2 [==============================] - 71s 35s/step - loss: 0.7569 - val_loss: 0.9457 - lr: 0.0100\n",
      "Epoch 19/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7584 \n",
      "Epoch 19: loss did not improve from 0.75664\n",
      "2/2 [==============================] - 72s 37s/step - loss: 0.7584 - val_loss: 1.0218 - lr: 0.0100\n",
      "Epoch 20/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7570 \n",
      "Epoch 20: loss did not improve from 0.75664\n",
      "2/2 [==============================] - 71s 35s/step - loss: 0.7570 - val_loss: 1.0447 - lr: 0.0100\n",
      "Epoch 21/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7558 \n",
      "Epoch 21: loss improved from 0.75664 to 0.75575, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 72s 36s/step - loss: 0.7558 - val_loss: 1.0374 - lr: 0.0100\n",
      "Epoch 22/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7566 \n",
      "Epoch 22: loss did not improve from 0.75575\n",
      "2/2 [==============================] - 68s 34s/step - loss: 0.7566 - val_loss: 1.0283 - lr: 0.0100\n",
      "Epoch 23/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7562 \n",
      "Epoch 23: loss did not improve from 0.75575\n",
      "2/2 [==============================] - 71s 35s/step - loss: 0.7562 - val_loss: 1.0033 - lr: 0.0100\n",
      "Epoch 24/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7568 \n",
      "Epoch 24: loss did not improve from 0.75575\n",
      "2/2 [==============================] - 68s 33s/step - loss: 0.7568 - val_loss: 1.0749 - lr: 0.0100\n",
      "Epoch 25/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7556 \n",
      "Epoch 25: loss improved from 0.75575 to 0.75559, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 70s 38s/step - loss: 0.7556 - val_loss: 1.0892 - lr: 0.0100\n",
      "Epoch 26/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7557 \n",
      "Epoch 26: loss did not improve from 0.75559\n",
      "2/2 [==============================] - 67s 33s/step - loss: 0.7557 - val_loss: 0.9773 - lr: 0.0100\n",
      "Epoch 27/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7557 \n",
      "Epoch 27: loss did not improve from 0.75559\n",
      "2/2 [==============================] - 66s 32s/step - loss: 0.7557 - val_loss: 0.9532 - lr: 0.0100\n",
      "Epoch 28/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7559 \n",
      "Epoch 28: loss did not improve from 0.75559\n",
      "2/2 [==============================] - 66s 34s/step - loss: 0.7559 - val_loss: 0.9358 - lr: 0.0100\n",
      "Epoch 29/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7554 \n",
      "Epoch 29: loss improved from 0.75559 to 0.75543, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 67s 34s/step - loss: 0.7554 - val_loss: 0.9215 - lr: 0.0100\n",
      "Epoch 30/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7543 \n",
      "Epoch 30: loss improved from 0.75543 to 0.75429, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 68s 35s/step - loss: 0.7543 - val_loss: 1.0337 - lr: 0.0100\n",
      "Epoch 31/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7540 \n",
      "Epoch 31: loss improved from 0.75429 to 0.75397, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 65s 34s/step - loss: 0.7540 - val_loss: 1.0098 - lr: 0.0100\n",
      "Epoch 32/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7557 \n",
      "Epoch 32: loss did not improve from 0.75397\n",
      "2/2 [==============================] - 66s 34s/step - loss: 0.7557 - val_loss: 1.0056 - lr: 0.0100\n",
      "Epoch 33/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7549 \n",
      "Epoch 33: loss did not improve from 0.75397\n",
      "2/2 [==============================] - 65s 32s/step - loss: 0.7549 - val_loss: 1.0350 - lr: 0.0100\n",
      "Epoch 34/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7542 \n",
      "Epoch 34: loss did not improve from 0.75397\n",
      "2/2 [==============================] - 70s 36s/step - loss: 0.7542 - val_loss: 0.9884 - lr: 0.0100\n",
      "Epoch 35/100000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.7547 \n",
      "Epoch 35: loss did not improve from 0.75397\n",
      "2/2 [==============================] - 72s 37s/step - loss: 0.7547 - val_loss: 1.0236 - lr: 0.0100\n",
      "Epoch 36/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7540 \n",
      "Epoch 36: loss improved from 0.75397 to 0.75395, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 76s 38s/step - loss: 0.7540 - val_loss: 0.9829 - lr: 0.0100\n",
      "Epoch 37/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7538 \n",
      "Epoch 37: loss improved from 0.75395 to 0.75384, saving model to my_best_model_v08.hdf5\n",
      "2/2 [==============================] - 69s 36s/step - loss: 0.7538 - val_loss: 1.1121 - lr: 0.0100\n",
      "Epoch 38/100000000\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7543 \n",
      "Epoch 38: loss did not improve from 0.75384\n",
      "2/2 [==============================] - 74s 38s/step - loss: 0.7543 - val_loss: 1.0629 - lr: 0.0100\n",
      "Epoch 39/100000000\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'SGD/SGD/update_168/ResourceApplyGradientDescent' defined at (most recent call last):\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\WEBFLA~2\\AppData\\Local\\Temp/ipykernel_24172/1272310905.py\", line 8, in <module>\n      history_full = model.fit(trainX_log, trainY_log, batch_size = batch_size, epochs = 100000000, verbose=1, shuffle=False, validation_data=(testX_log, testY_log), callbacks=callbacks)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 532, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 671, in apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 716, in _distributed_apply\n      update_op = distribution.extended.update(\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 699, in apply_grad_to_update_var\n      update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\gradient_descent.py\", line 142, in _resource_apply_dense\n      return tf.raw_ops.ResourceApplyGradientDescent(\nNode: 'SGD/SGD/update_168/ResourceApplyGradientDescent'\nOOM when allocating tensor with shape[21504,2048] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node SGD/SGD/update_168/ResourceApplyGradientDescent}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_41290]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WEBFLA~2\\AppData\\Local\\Temp/ipykernel_24172/1272310905.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearlystopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrlrop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mhistory_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX_log\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY_log\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX_log\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# checkpoint365 = ModelCheckpoint(filepath='my_best_model_365.hdf5', monitor='loss', verbose=1, save_best_only=True, mode='min')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'SGD/SGD/update_168/ResourceApplyGradientDescent' defined at (most recent call last):\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\WEBFLAX-FOUR\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\WEBFLA~2\\AppData\\Local\\Temp/ipykernel_24172/1272310905.py\", line 8, in <module>\n      history_full = model.fit(trainX_log, trainY_log, batch_size = batch_size, epochs = 100000000, verbose=1, shuffle=False, validation_data=(testX_log, testY_log), callbacks=callbacks)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 532, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 671, in apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 716, in _distributed_apply\n      update_op = distribution.extended.update(\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 699, in apply_grad_to_update_var\n      update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\n    File \"C:\\Users\\WEBFLAX-FOUR\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\gradient_descent.py\", line 142, in _resource_apply_dense\n      return tf.raw_ops.ResourceApplyGradientDescent(\nNode: 'SGD/SGD/update_168/ResourceApplyGradientDescent'\nOOM when allocating tensor with shape[21504,2048] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node SGD/SGD/update_168/ResourceApplyGradientDescent}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_41290]"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='loss', patience=200, restore_best_weights=True)\n",
    "rlrop = ReduceLROnPlateau(monitor='loss', patience=30, factor=0.5, min_lr=0.000001)\n",
    "checkpoint_full = ModelCheckpoint(filepath='my_best_model_v08.hdf5', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint_full, earlystopping, rlrop]\n",
    "\n",
    "history_full = model.fit(trainX_log, trainY_log, batch_size = batch_size, epochs = 100000000, verbose=1, shuffle=False, validation_data=(testX_log, testY_log), callbacks=callbacks)\n",
    "\n",
    "# checkpoint365 = ModelCheckpoint(filepath='my_best_model_365.hdf5', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks = [checkpoint365, earlystopping]\n",
    "# history365 = regressor.fit(trainX[(len(trainY)-365):], trainY[(len(trainY)-365):], batch_size = batch_size, epochs = 50000, verbose=1, shuffle=False, validation_data=(testX, testY), callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef7aa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:24:46.325426Z",
     "iopub.status.busy": "2021-11-09T08:24:46.324389Z",
     "iopub.status.idle": "2021-11-09T08:24:46.568908Z",
     "shell.execute_reply": "2021-11-09T08:24:46.569365Z",
     "shell.execute_reply.started": "2021-11-09T07:54:23.288972Z"
    },
    "id": "75ef7aa9",
    "papermill": {
     "duration": 0.46139,
     "end_time": "2021-11-09T08:24:46.569557",
     "exception": false,
     "start_time": "2021-11-09T08:24:46.108167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.plot(history_full.history['loss'], label='train')\n",
    "\n",
    "plt.plot(history_full.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b511a52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:24:35.370435Z",
     "iopub.status.busy": "2021-11-09T08:24:35.369774Z",
     "iopub.status.idle": "2021-11-09T08:24:45.892279Z",
     "shell.execute_reply": "2021-11-09T08:24:45.892858Z",
     "shell.execute_reply.started": "2021-11-09T07:54:22.907318Z"
    },
    "id": "7b511a52",
    "papermill": {
     "duration": 10.74732,
     "end_time": "2021-11-09T08:24:45.893042",
     "exception": false,
     "start_time": "2021-11-09T08:24:35.145722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_from_saved_checkpoint = load_model('my_best_model_full.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2JHDgEvC8Jzr",
   "metadata": {
    "id": "2JHDgEvC8Jzr"
   },
   "source": [
    "# test test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "hp_Re5CnPF8b",
   "metadata": {
    "id": "hp_Re5CnPF8b"
   },
   "outputs": [],
   "source": [
    "# batch_size = 1\n",
    "\n",
    "\n",
    "# model_from_saved_checkpoint = Sequential()\n",
    "\n",
    "\n",
    "# model_from_saved_checkpoint.add(LSTM(units = 256, activation = 'relu', return_sequences=True, batch_input_shape = (batch_size, trainX.shape[1], trainX.shape[2]), stateful=True)) #  dropout=0.25, recurrent_dropout=0.1,\n",
    "# model_from_saved_checkpoint.add(Dropout(0.2))\n",
    "\n",
    "# model_from_saved_checkpoint.add(LSTM(units = 512, activation = 'relu', return_sequences=True, batch_input_shape = (batch_size, trainX.shape[1], trainX.shape[2]), stateful=True)) #  dropout=0.25, recurrent_dropout=0.1,\n",
    "# model_from_saved_checkpoint.add(Dropout(0.3))\n",
    "\n",
    "# model_from_saved_checkpoint.add(LSTM(units = 1024, activation = 'relu', return_sequences=True, batch_input_shape = (batch_size, trainX.shape[1], trainX.shape[2]), stateful=True)) #  dropout=0.25, recurrent_dropout=0.1,\n",
    "# model_from_saved_checkpoint.add(Dropout(0.4))\n",
    "\n",
    "# model_from_saved_checkpoint.add(LSTM(units = 128, activation = 'relu', batch_input_shape = (batch_size, trainX.shape[1], trainX.shape[2]), stateful=True))\n",
    "# model_from_saved_checkpoint.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# model_from_saved_checkpoint.add(Dense(units = 32, activation = 'relu'))\n",
    "# model_from_saved_checkpoint.add(Dropout(0.3))\n",
    "\n",
    "# model_from_saved_checkpoint.add(Dense(units = 1))\n",
    "\n",
    "\n",
    "# model_from_saved_checkpoint.load_weights('my_best_model_full.hdf5')\n",
    "    \n",
    "# model_from_saved_checkpoint.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HjGEgA8-u5Bj",
   "metadata": {
    "id": "HjGEgA8-u5Bj"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "\n",
    "# log\n",
    "predicted_btc_price_test_data = model_from_saved_checkpoint.predict(testX)\n",
    "predicted_btc_price_test_data = scaler_tlog.inverse_transform(np.array(predicted_btc_price_test_data).reshape(-1, 1))\n",
    "test_actual = scaler_tlog.inverse_transform(testY.reshape(-1, 1))\n",
    "\n",
    "plt.plot(np.exp(predicted_btc_price_test_data), 'r', marker='.', label='Predicted Test')\n",
    "plt.plot(np.exp(test_actual), marker='.', label='Actual Test')\n",
    "\n",
    "# # manoal\n",
    "# predicted_btc_price_test_data = model_from_saved_checkpoint.predict(testX)\n",
    "# predicted_btc_price_test_data = scaler_t.inverse_transform(predicted_btc_price_test_data.reshape(-1, 1))\n",
    "# test_actual = scaler_t.inverse_transform(testY.reshape(-1, 1))\n",
    "\n",
    "# plt.plot(predicted_btc_price_test_data, 'r', marker='.', label='Predicted Test')\n",
    "# plt.plot(test_actual, marker='.', label='Actual Test')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t0-jxnhq79Rz",
   "metadata": {
    "id": "t0-jxnhq79Rz"
   },
   "source": [
    "#test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IfHVjwzz6Eqw",
   "metadata": {
    "id": "IfHVjwzz6Eqw"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,7))\n",
    "\n",
    "# log\n",
    "predicted_btc_price_train_data = model_from_saved_checkpoint.predict(trainX)\n",
    "predicted_btc_price_train_data = scaler_tlog.inverse_transform(np.array(predicted_btc_price_train_data).reshape(-1, 1))\n",
    "train_actual = scaler_tlog.inverse_transform(trainY.reshape(-1, 1))\n",
    "\n",
    "plt.plot(np.exp(predicted_btc_price_train_data), 'r', marker='.', label='Predicted Test')\n",
    "plt.plot(np.exp(train_actual), marker='.', label='Actual Test')\n",
    "\n",
    "# # manoal\n",
    "# predicted_btc_price_train_data = model_from_saved_checkpoint.predict(trainX)\n",
    "# predicted_btc_price_train_data = scaler_t.inverse_transform(predicted_btc_price_train_data.reshape(-1, 1))\n",
    "# train_actual = scaler_t.inverse_transform(trainY.reshape(-1, 1))\n",
    "\n",
    "# plt.plot(predicted_btc_price_train_data, 'r', marker='.', label='Predicted Test')\n",
    "# plt.plot(train_actual, marker='.', label='Actual Test')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RbESQNif7sFE",
   "metadata": {
    "id": "RbESQNif7sFE"
   },
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VfWzmxzRmWj9",
   "metadata": {
    "id": "VfWzmxzRmWj9"
   },
   "source": [
    "## log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528eac3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T08:24:51.673734Z",
     "iopub.status.busy": "2021-11-09T08:24:51.672794Z",
     "iopub.status.idle": "2021-11-09T08:24:51.678780Z",
     "shell.execute_reply": "2021-11-09T08:24:51.679241Z",
     "shell.execute_reply.started": "2021-11-09T07:54:24.941735Z"
    },
    "id": "f528eac3",
    "outputId": "6214d540-9a07-421f-a738-5f83c158d2e4",
    "papermill": {
     "duration": 0.229983,
     "end_time": "2021-11-09T08:24:51.679408",
     "exception": false,
     "start_time": "2021-11-09T08:24:51.449425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 2363.389\n"
     ]
    }
   ],
   "source": [
    "rmse_lstm_test = math.sqrt(mean_squared_error(np.exp(test_actual), np.exp(predicted_btc_price_test_data)))\n",
    "\n",
    "print('Test RMSE: %.3f' % rmse_lstm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694569c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T08:24:52.578248Z",
     "iopub.status.busy": "2021-11-09T08:24:52.577170Z",
     "iopub.status.idle": "2021-11-09T08:24:52.583165Z",
     "shell.execute_reply": "2021-11-09T08:24:52.583695Z",
     "shell.execute_reply.started": "2021-11-09T07:54:24.950091Z"
    },
    "id": "4694569c",
    "outputId": "e6a22ae5-0f2a-47bb-a7cb-e22fab6d0934",
    "papermill": {
     "duration": 0.235012,
     "end_time": "2021-11-09T08:24:52.583860",
     "exception": false,
     "start_time": "2021-11-09T08:24:52.348848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 1764.024\n"
     ]
    }
   ],
   "source": [
    "rmse_lstm_train = math.sqrt(mean_squared_error(np.exp(train_actual), np.exp(predicted_btc_price_train_data)))\n",
    "\n",
    "print('Train RMSE: %.3f' % rmse_lstm_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EQivOH7Hmdym",
   "metadata": {
    "id": "EQivOH7Hmdym"
   },
   "source": [
    "## manoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GwD32lLQmj9V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T08:24:51.673734Z",
     "iopub.status.busy": "2021-11-09T08:24:51.672794Z",
     "iopub.status.idle": "2021-11-09T08:24:51.678780Z",
     "shell.execute_reply": "2021-11-09T08:24:51.679241Z",
     "shell.execute_reply.started": "2021-11-09T07:54:24.941735Z"
    },
    "id": "GwD32lLQmj9V",
    "outputId": "ba8e84ee-d45f-492f-f6ad-45a3cff9dd99",
    "papermill": {
     "duration": 0.229983,
     "end_time": "2021-11-09T08:24:51.679408",
     "exception": false,
     "start_time": "2021-11-09T08:24:51.449425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 2951.213\n"
     ]
    }
   ],
   "source": [
    "rmse_lstm_test = math.sqrt(mean_squared_error(test_actual, predicted_btc_price_test_data))\n",
    "\n",
    "print('Test RMSE: %.3f' % rmse_lstm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SM8YFgpmmj9W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T08:24:52.578248Z",
     "iopub.status.busy": "2021-11-09T08:24:52.577170Z",
     "iopub.status.idle": "2021-11-09T08:24:52.583165Z",
     "shell.execute_reply": "2021-11-09T08:24:52.583695Z",
     "shell.execute_reply.started": "2021-11-09T07:54:24.950091Z"
    },
    "id": "SM8YFgpmmj9W",
    "outputId": "3a19fa3a-8351-4cb7-87de-b1194fa1544b",
    "papermill": {
     "duration": 0.235012,
     "end_time": "2021-11-09T08:24:52.583860",
     "exception": false,
     "start_time": "2021-11-09T08:24:52.348848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1503.741\n"
     ]
    }
   ],
   "source": [
    "rmse_lstm_train = math.sqrt(mean_squared_error(train_actual, predicted_btc_price_train_data))\n",
    "\n",
    "print('Train RMSE: %.3f' % rmse_lstm_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4XLVI0GT8Ccb",
   "metadata": {
    "id": "4XLVI0GT8Ccb"
   },
   "source": [
    "# future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Ff93g4Ab3Xi",
   "metadata": {
    "id": "0Ff93g4Ab3Xi"
   },
   "source": [
    "###### genertor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "nSQ-YaQi12eP",
   "metadata": {
    "id": "nSQ-YaQi12eP"
   },
   "outputs": [],
   "source": [
    "# def rsi():\n",
    "#   return scaler_rsi.transform(np.array(ta.rsi(pd.Series(data=np.exp(scaler_tlog.inverse_transform(np.array(c_log).reshape(-1,1))).reshape(-1)),length=14))[-1].reshape(-1,1)).reshape(-1,1)\n",
    "\n",
    "def ema20():\n",
    "  return scaler_t.transform(np.array(np.array(ta.ma(\"ema\", pd.Series(data=np.exp(scaler_tlog.inverse_transform(np.array(c_log).reshape(-1,1))).reshape(-1)),length=20))[-1].reshape(-1,1))).reshape(-1,1)\n",
    "\n",
    "def ema50():\n",
    "  return scaler_t.transform(np.array(np.array(ta.ma(\"ema\", pd.Series(data=np.exp(scaler_tlog.inverse_transform(np.array(c_log).reshape(-1,1))).reshape(-1)),length=50))[-1].reshape(-1,1))).reshape(-1,1)\n",
    "\n",
    "def ema100():\n",
    "  return scaler_t.transform(np.array(np.array(ta.ma(\"ema\", pd.Series(data=np.exp(scaler_tlog.inverse_transform(np.array(c_log).reshape(-1,1))).reshape(-1)),length=100))[-1].reshape(-1,1))).reshape(-1,1)\n",
    "\n",
    "def ema200():\n",
    "  return scaler_t.transform(np.array(np.array(ta.ma(\"ema\", pd.Series(data=np.exp(scaler_tlog.inverse_transform(np.array(c_log).reshape(-1,1))).reshape(-1)),length=200))[-1].reshape(-1,1))).reshape(-1,1)\n",
    "\n",
    "def ema300():\n",
    "  return scaler_t.transform(np.array(np.array(ta.ma(\"ema\", pd.Series(data=np.exp(scaler_tlog.inverse_transform(np.array(c_log).reshape(-1,1))).reshape(-1)),length=300))[-1].reshape(-1,1))).reshape(-1,1)\n",
    "\n",
    "\n",
    "def mom10():\n",
    "  return scaler_mom.transform(np.array(np.array(ta.mom(pd.Series(data=np.exp(scaler_tlog.inverse_transform(np.array(c_log).reshape(-1,1))).reshape(-1)),length=10))[-1].reshape(-1,1))).reshape(-1,1)\n",
    "\n",
    "# def mom30():\n",
    "#   return scaler_mom.transform(np.array(np.array(ta.mom(pd.Series(data=np.exp(scaler_tlog.inverse_transform(np.array(c_log).reshape(-1,1))).reshape(-1)),length=30))[-1].reshape(-1,1))).reshape(-1,1)\n",
    "\n",
    "def mom60():\n",
    "  return scaler_mom.transform(np.array(np.array(ta.mom(pd.Series(data=np.exp(scaler_tlog.inverse_transform(np.array(c_log).reshape(-1,1))).reshape(-1)),length=60))[-1].reshape(-1,1))).reshape(-1,1)\n",
    "\n",
    "# def mom90():\n",
    "#   return scaler_mom.transform(np.array(np.array(ta.mom(pd.Series(data=np.exp(scaler_tlog.inverse_transform(np.array(c_log).reshape(-1,1))).reshape(-1)),length=90))[-1].reshape(-1,1))).reshape(-1,1)\n",
    "\n",
    "\n",
    "# def diff1_log(): # target log\n",
    "#   d_old = scaler_tlog.inverse_transform(np.array(c_log)[-1].reshape(-1,1))\n",
    "#   d_new = scaler_tlog.inverse_transform(np.array(per).reshape(-1,1)).reshape(-1,1)\n",
    "#   d = scaler_diff_log.transform(np.array(d_new - d_old).reshape(-1,1))\n",
    "#   return d\n",
    "\n",
    "# def diff7_log(): # target log\n",
    "#   d_old = scaler_tlog.inverse_transform(np.array(c_log)[-7].reshape(-1,1))\n",
    "#   d_new = scaler_tlog.inverse_transform(np.array(per).reshape(-1,1)).reshape(-1,1)\n",
    "#   d = scaler_diff_log.transform(np.array(d_new - d_old).reshape(-1,1))\n",
    "#   return d\n",
    "\n",
    "# def diff30_log(): # target log\n",
    "#   d_old = scaler_tlog.inverse_transform(np.array(c_log)[-30].reshape(-1,1))\n",
    "#   d_new = scaler_tlog.inverse_transform(np.array(per).reshape(-1,1)).reshape(-1,1)\n",
    "#   d = scaler_diff_log.transform(np.array(d_new - d_old).reshape(-1,1))\n",
    "#   return d\n",
    "\n",
    "# def diff1(): # target log\n",
    "#   d_old = np.exp(scaler_tlog.inverse_transform(np.array(c_log)[-1].reshape(-1,1)))\n",
    "#   d_new = np.exp(scaler_tlog.inverse_transform(np.array(per).reshape(-1,1))).reshape(-1,1)\n",
    "#   d = scaler_diff.transform(np.array(d_new - d_old).reshape(-1,1))\n",
    "#   return d\n",
    "\n",
    "def diff7(): # target log\n",
    "  d_old = np.exp(scaler_tlog.inverse_transform(np.array(c_log)[-8].reshape(-1,1)))\n",
    "  d_new = np.exp(scaler_tlog.inverse_transform(np.array(per).reshape(-1,1))).reshape(-1,1)\n",
    "  d = scaler_diff.transform(np.array(d_new - d_old).reshape(-1,1))\n",
    "  return d\n",
    "\n",
    "# def diff20(): # target log\n",
    "#   d_old = np.exp(scaler_tlog.inverse_transform(np.array(c_log)[-21].reshape(-1,1)))\n",
    "#   d_new = np.exp(scaler_tlog.inverse_transform(np.array(per).reshape(-1,1))).reshape(-1,1)\n",
    "#   d = scaler_diff.transform(np.array(d_new - d_old).reshape(-1,1))\n",
    "#   return d\n",
    "\n",
    "def diff30(): # target log\n",
    "  d_old = np.exp(scaler_tlog.inverse_transform(np.array(c_log)[-31].reshape(-1,1)))\n",
    "  d_new = np.exp(scaler_tlog.inverse_transform(np.array(per).reshape(-1,1))).reshape(-1,1)\n",
    "  d = scaler_diff.transform(np.array(d_new - d_old).reshape(-1,1))\n",
    "  return d\n",
    "\n",
    "def diff60(): # target log\n",
    "  d_old = np.exp(scaler_tlog.inverse_transform(np.array(c_log)[-61].reshape(-1,1)))\n",
    "  d_new = np.exp(scaler_tlog.inverse_transform(np.array(per).reshape(-1,1))).reshape(-1,1)\n",
    "  d = scaler_diff.transform(np.array(d_new - d_old).reshape(-1,1))\n",
    "  return d\n",
    "\n",
    "#   def diff90(): # target log\n",
    "#   d_old = np.exp(scaler_tlog.inverse_transform(np.array(c_log)[-91].reshape(-1,1)))\n",
    "#   d_new = np.exp(scaler_tlog.inverse_transform(np.array(per).reshape(-1,1))).reshape(-1,1)\n",
    "#   d = scaler_diff.transform(np.array(d_new - d_old).reshape(-1,1))\n",
    "#   return d\n",
    "\n",
    "\n",
    "def close():\n",
    "  return scaler_t.transform(np.array(np.exp(scaler_tlog.inverse_transform(np.array(per).reshape(-1,1)))).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HvDqw9rEvlp1",
   "metadata": {
    "id": "HvDqw9rEvlp1"
   },
   "source": [
    "###### many to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "tSdGaneS3zms",
   "metadata": {
    "id": "tSdGaneS3zms"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# maxlen = 400\n",
    "# c_log = deque(maxlen=maxlen)\n",
    "\n",
    "# for i in range(-maxlen,0,1):\n",
    "#   # prev_days.append(scaler.transform(test_actual[i].reshape(-1,1)))\n",
    "#   c_log.append(look_b['close_log'].values[i])\n",
    "\n",
    "\n",
    "SEQ_LEN=len(testX[-1])\n",
    "prev_days = deque(maxlen=SEQ_LEN)\n",
    "\n",
    "for i in range(-len(testX[-1]),0,1):\n",
    "  # prev_days.append(scaler.transform(test_actual[i].reshape(-1,1)))\n",
    "  prev_days.append(testX[-1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "PhUAYS2YV3jo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhUAYS2YV3jo",
    "outputId": "96bcb42a-8f88-4eaf-cc45-5785dcb0e62f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['close_log', 'close', 'target_log', 'target'], dtype='<U10')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(col).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "mZjpKhwh1qI6",
   "metadata": {
    "id": "mZjpKhwh1qI6"
   },
   "outputs": [],
   "source": [
    "y_p_ = []\n",
    "future_len = 300\n",
    "for i in range(future_len):\n",
    "  x__ = np.array(prev_days).reshape(-1,len(np.array(prev_days)[-1])).astype(np.float32)\n",
    "\n",
    "  per = model_from_saved_checkpoint.predict(np.expand_dims(x__, axis=0))\n",
    "\n",
    "  # Close = close()\n",
    "  # # Rsi = rsi()\n",
    "  # Mom10 = mom10()\n",
    "  # # Mom30 = mom30()\n",
    "  # Mom60 = mom60()\n",
    "  # # Mom90 = mom90()\n",
    "  # # Diff1_log = diff1_log()\n",
    "  # # Diff7_log = diff7_log()\n",
    "  # # Diff30_log = diff30_log()\n",
    "  # # Diff1 = diff1()\n",
    "  # Diff7 = diff7()\n",
    "  # # Diff20 = diff20()\n",
    "  # Diff30 = diff30()\n",
    "  # Diff60 = diff60()\n",
    "  # # Diff90 = diff90()\n",
    "  # Ema20 = ema20()\n",
    "  # Ema50 = ema50()\n",
    "  # Ema100 = ema100()\n",
    "  # Ema200 = ema200()\n",
    "  # Ema300 = ema300()\n",
    "\n",
    "  prev_days.append(np.array(per).reshape(-1,1)) \n",
    "\n",
    "  # prev_days.append(np.array([per.reshape(-1,1), Close, Diff7, Diff30, Diff60, Ema20, Ema50, Ema100, Ema200, Ema300, Mom10]).reshape(-1,)) # Diff30, Diff60, Diff90, Mom30, Mom60, Mom90]) # , Diff1_log, Diff7_log, Diff30_log, Diff1, Diff7, Rsi,\n",
    "  # c_log.append(np.array(per).reshape(-1,1))\n",
    "\n",
    "  y_p_.append(per.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "wd_Ho0eV1_G8",
   "metadata": {
    "id": "wd_Ho0eV1_G8"
   },
   "outputs": [],
   "source": [
    "# log\n",
    "y_p_ = scaler_tlog.inverse_transform(np.array(y_p_).reshape(-1, 1))\n",
    "# manoal\n",
    "# y_p_ = scaler_t.inverse_transform(np.array(y_p_).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zCLDjXpAZq08",
   "metadata": {
    "id": "zCLDjXpAZq08"
   },
   "source": [
    "###### plot future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "RcXNZ_OckkuZ",
   "metadata": {
    "id": "RcXNZ_OckkuZ"
   },
   "outputs": [],
   "source": [
    "future_actual = scaler_tlog.inverse_transform(futureY.reshape(-1, 1))\n",
    "actual = np.concatenate((train_actual,test_actual,future_actual))\n",
    "predicted_btc_price = np.concatenate((predicted_btc_price_train_data, predicted_btc_price_test_data, np.array(y_p_).reshape(-1,1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe8243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:24:50.553036Z",
     "iopub.status.busy": "2021-11-09T08:24:50.548756Z",
     "iopub.status.idle": "2021-11-09T08:24:50.791501Z",
     "shell.execute_reply": "2021-11-09T08:24:50.791940Z",
     "shell.execute_reply.started": "2021-11-09T07:54:24.729679Z"
    },
    "id": "68fe8243",
    "papermill": {
     "duration": 0.481274,
     "end_time": "2021-11-09T08:24:50.792100",
     "exception": false,
     "start_time": "2021-11-09T08:24:50.310826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,7))\n",
    "\n",
    "# log\n",
    "plt.plot(np.exp(actual),'y', marker='.', label='Actual Train')\n",
    "plt.plot(np.exp(predicted_btc_price), 'g', marker='.', label='Predicted Train')\n",
    "# manoal\n",
    "# plt.plot(actual,'y', marker='.', label='Actual Train')\n",
    "# plt.plot(predicted_btc_price, 'g', marker='.', label='Predicted Train')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "H1F56lS_93TS",
    "HfXgO1eM90Iq",
    "rqK_xrJWVuck",
    "RbESQNif7sFE",
    "VfWzmxzRmWj9",
    "EQivOH7Hmdym",
    "0Ff93g4Ab3Xi",
    "HvDqw9rEvlp1"
   ],
   "name": "btc_V0.8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 155.774638,
   "end_time": "2021-11-09T08:24:56.343239",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-09T08:22:20.568601",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
